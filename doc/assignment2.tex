\documentclass{article}
\usepackage{a4wide}
\usepackage{hyperref}
%\setlength{\parindent}{0pt}
\title{Web Data Management, assignment 2}
\author{Hans van den Bogert\\1307983 \and Bastiaan van Graafeiland\\1399101}
\begin{document}
\maketitle

\section{Introduction}
This document describes the choices made for assignment 2 which in our
case was chapter 19 from: "Web Data Management" by Serge
Abiteboul et al.

\section{Exercise 19.5.1 }
For this exercise we basically copied the source-code from the example
(as was implied by the exercise) and added the \texttt{Authors.CountReducer}
class as the combiner. This works fine because the counting of author names is associative
and commutative.

\section{Exercise 19.5.2}
Normally hadoop by default works on a granularity of lines for the
mappers. In the case of XML-data this does not work. Fortunately a lot
of other people have already solved this by using Hadoops extensible
\texttt{InputFormat}. In particular we used the class
\texttt{XMLInputFormat} from the \emph{Cloud9} Hadoop toolkit, which
only needed minor alterations to work in our setting. Now by using
this input formatter, the mappers work on the granularity of enclosing
\texttt{movie} tags.

\section{Exercise 19.5.3}
For each query, we created a separate Pig script file. These can be found in \texttt{src/main/pig}.

\section{Exercise 19.5.4}
The inverted file is built using a single MapReduce job:
\texttt{InverseDocument.java}. The input needs to be a collection
of plain text files representing the documents. The output is then in the format of
\texttt{term (tab) total\_tf:(number); idf:(idf value); filename1:(number of occurrences in filename1); filename2: ...}.

For building the index, no stop words are used (every word counts). In addition, we convert each word to lower case
and strip it of any punctuation or quotes.

We also used a separate Combiner for this job, because the output of the Reducer is different than that of the Mapper.

We use the raw term frequency per document, and the default calculation of idf:
$$ \mathrm{idf}(t, D) = \log \frac{|D|}{|\{d \in D: t \in d\}|} $$

\section{Running the code}
To download the necessary resources, execute \texttt{download-resources.sh}, which can be
found in the \texttt{res} directory.

Maven is best used to execute the jobs. Run \texttt{maven package},
then \texttt{mvn exec:java -Dexec.mainClass="\{class to use\}" -Dexec.args="\{required arguments\}"}.

As a convenience, a \texttt{Vagrantfile} has been added which can be
used with the \url{vagrantup.com} framework for starting Virtual Machines
locally to be used as a build platform. 

Once the VM is started with \texttt{\$ vagrant up} (you will have to
install both \emph{Vagrant} and \emph{Virtualbox}) you can then login
to the machine with \texttt{\$ vagrant ssh} and afterwards \texttt{\$
  cd /vagrant} to access the code base from within the virtual machine 
\end{document}
